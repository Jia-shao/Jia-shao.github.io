<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yijia Wang</title>

    <meta name="author" content="Yijia Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yijia Wang
                </p>
                <p>
					Hi, I am a final-year undergraduate student at <a href="https://nkdailab.github.io/">the Artificial Intelligence Lab</a> of <a href="https://eee.sustech.edu.cn/">Southern University of Science and Technology</a>, 
					under the supervision of <a href="https://nkdailab.github.io/author/%E4%BD%95%E5%BF%97%E6%B5%B7/">Professor Zhihai He</a> (IEEE Fellow). 
					I also spent an impressive summer in 2024 at <a href="https://www.stanford.edu/">Stanford University</a>, where I deepened my understanding of algorithms and optimizations.
                </p>
				<p>
					My research is focused on image generation, and its practical applications, including image coding and image super-resolution. 
					I am also interested in developing efficient image generation models that offer faster speed and higher quality. 
					My goal is to address key challenges in the image generation process, thereby improving the performance of downstream tasks that are based on generation methods.
				</p>
				<p>
					<strong>Currently, I am seeking PhD opportunities</strong> to further explore the key issues in image generation technology. 
					I aim to develop advanced algorithms that can significantly enhance the quality and speed of image generation, 
					and I am eager to apply these techniques to real-world scenarios to make a positive impact.
				</p>
                <p style="text-align:center">
                  <a href="mailto:wangyijia1234@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!--<a href="data/YijiaWang_cv.pdf">CV</a> &nbsp;/&nbsp;-->
                  <!--<a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=ffdw8pMAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <!--<a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/Jia-shao/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%;  height:20%; max-height:20%; position: relative;">
                <a href="images/YijiaWang_image2.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/YijiaWang_image2.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
			
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:16px;width:100%;vertical-align:middle">
            	<h2>News</h2>
                	<p>
                		[11/04/2025] Congratulations! I have been awarded the Guo Xie Birong Scholarship. This is a significant achievement, and I’m very grateful for the recognition of my academic efforts.
                	</p>
					<p>
						[10/27/2025] Congratulations! Our team has won the Silver Award in the 2025 China International University Students Innovation and Entrepreneurship Competition of Guangdong Province. Among the 484,500 teams that participated, only 234 teams (Top 0.07%) received this prestigious award.
					</p>
				<p>
					[09/09/2025] Two of my patents have been accepted for examination.
				</p>
            </td>
        </tr>
    </tbody></table>
    
	<table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
            	<h2>Research Experience</h2>
				<p>
					(* denotes equal contributions.)
				</p>
            </td>
        </tr>
    </tbody></table>
			  
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:20px;width:25%;vertical-align:middle">
            	<img src='images/GSC1.png' width="200">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:middle">
          		<p style="color: black; font-weight: bold; font-size: 1.2em;">  
              		<papertitle>Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis</papertitle>
		  		</p>
            	<a href="https://weimingchen.github.io/">Weiming Chen*</a>,
            	<strong>Yijia Wang*</strong>,
            	<a href="https://airtnzhuzh.github.io/">Zhihan Zhu</a>,   
            	<a href="https://scholar.google.com/citations?hl=zh-CN&user=wtr6OgkAAAAJ">Zhihai He</a>, 
           		<!--(* for equal contribution, <sup>&dagger;</sup> for project lead)-->
            	<br>
           		<em>Under review</em>
            	<br>
            	<a href="http://arxiv.org/abs/2510.27324">[PDF]</a>
	            <!--  /
	            <a href="https://ai4ce.github.io/INT-ACT/">Project Page</a>
	            /
	            <a href="https://github.com/ai4ce/INT-ACT">Code</a>
	            /
	            <a href="https://huggingface.co/collections/ai4ce/intact-probing-suite-684e5601e9ed640fdd9b994b">HuggingFace</a> -->
	            <p>Text + tiny latent steer a generative model to reconstruct images at ultra-low bitrates.</p>
	            <!--<p>Text + tiny latent steer a generative model to reconstruct images at ultra-low bitrates.</p>-->
          </td>
        </tr>
	 </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:20px;width:25%;vertical-align:middle">
            	<img src='images/RK_DDTA.png' width="200">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:middle">
          		<p style="color: black; font-weight: bold; font-size: 1.2em;">  
              		<papertitle>Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</papertitle>
		  		</p>
            	<a href="https://weimingchen.github.io/">Weiming Chen</a>,
				<a href="https://airtnzhuzh.github.io/">Zhihan Zhu</a>,
            	<strong>Yijia Wang</strong>,
            	<a href="https://scholar.google.com/citations?hl=zh-CN&user=wtr6OgkAAAAJ">Zhihai He</a>, 
           		<!--(* for equal contribution, <sup>&dagger;</sup> for project lead)-->
            	<br>
           		<em>Under review</em>
            	<br>
            	<a href="https://arxiv.org/abs/2509.12888">[PDF]</a>
	            <!--  /
	            <a href="https://ai4ce.github.io/INT-ACT/">Project Page</a>
	            /
	            <a href="https://github.com/ai4ce/INT-ACT">Code</a>
	            /
	            <a href="https://huggingface.co/collections/ai4ce/intact-probing-suite-684e5601e9ed640fdd9b994b">HuggingFace</a> -->
	            <p>
					We improved Rectified Flow editing via a Runge-Kutta solver for accurate inversion and decoupled attention for precise control.
				</p>
	            <!--<p>When building a VLA from VLMs, how much generalization power can it get?</p>-->
          </td>
        </tr>
	 </tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:20px;width:25%;vertical-align:middle">
            	<img src='images/reasoning_editing.png' width="200">
          	</td>
          	<td style="padding:20px;width:75%;vertical-align:middle">
          		<p style="color: black; font-weight: bold; font-size: 1.2em;">  
              		<papertitle>Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing</papertitle>
		  		</p>
				<strong>Yijia Wang</strong>,
				Yiqing Shen,
            	<a href="https://weimingchen.github.io/">Weiming Chen</a>,
            	<a href="https://scholar.google.com/citations?hl=zh-CN&user=wtr6OgkAAAAJ">Zhihai He</a>, 
           		<!--(* for equal contribution, <sup>&dagger;</sup> for project lead)-->
            	<br>
           		<em>Under review</em>
            	<br>
            	<a href="http://arxiv.org/abs/2510.27335">[PDF]</a>
	            <!--  /
	            <a href="https://ai4ce.github.io/INT-ACT/">Project Page</a>
	            /
	            <a href="https://github.com/ai4ce/INT-ACT">Code</a>
	            /
	            <a href="https://huggingface.co/collections/ai4ce/intact-probing-suite-684e5601e9ed640fdd9b994b">HuggingFace</a> -->
	            <p>
					We proposed CIELR for complex image editing by using LLMs to reason about implicit instructions and convert them into simple editing steps, all without fine-tuning.
				</p>
	            <!--<p>When building a VLA from VLMs, how much generalization power can it get?</p>-->
          </td>
        </tr>
	 </tbody></table>

	<!--<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
    	<tr>
        	<td>
            	<h2>Education</h2>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Education</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:16px;width:100%;vertical-align:middle">
            	<h2>Education</h2>
            </td>
        </tr>
          </tbody></table>
			
	<table width="100%" align="center" border="0" cellpadding="10">
    	<tbody>
        	<tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/logo/stanford.png" ,="" width="100%"></td>
              <td width="80%" valign="center">
                <b>Stanford University</b>
                <br> 06/2024 - 08/2024, Stanford, California <br>
                <br> <b>Exchange Student</b>
				<br> GPA: 4.00/4.00
              </td>
            </tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/logo/sustech.png" ,="" width="100%"></td>
              <td width="80%" valign="center">
                <b>Southern University of Science and Technology</b>
                <br> 08/2022 - present, Shenzhen, China <br>
                <br> <b>Bachelor of Engineering in Electronic and Electrical Engineering</b>
                <br> GPA: 3.90/4.00
                <br> Advisor: <a href="https://nkdailab.github.io/author/%E4%BD%95%E5%BF%97%E6%B5%B7/">Professor Zhihai He</a>
              </td>
            </tr>
          </tbody>
        </table>
			
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    	<tr>
        	<td style="padding:16px;width:100%;vertical-align:middle">
            	<h2>Scholarships/Awards</h2>
				<p>
					[11/2025] 2024-2025 BYD Scholarship
				</p>
				<p>
					[09/2025] 2024-2025 Guo Xie Birong Scholarship
				</p>
				<p>
					[09/2025] 2024-2025 First-Class Scholarship for Outstanding Students, SUSTech
				</p>
				<p>
					[08/2025] Silver Award in 2025 China International University Students Innovation and Entrepreneurship Competition, Guangdong
				</p>
				<p>
					[09/2024] 2023-2024 Second-Classs Scholarship for Outstanding Students, SUSTech
				</p>
				<p>
					[01/2024] Second Prize in the 2023 China Undergraduate Mathematical Contest in Modeling, Guangdong
				</p>
				<p>
					[09/2023] 2022-2023 Star of Practice Scholarship, SUSTech
				</p>
				<p>
					[09/2023] 2022-2023 First-Class Scholarship for Outstanding Students, SUSTech
				</p>
            </td>
        </tr>
    </tbody></table>
			
		<!-- 尾注	  
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table> 
-->

        </td>
      </tr>
    </table>
  </body>
</html>
